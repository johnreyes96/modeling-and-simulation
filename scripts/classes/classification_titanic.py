# -*- coding: utf-8 -*-
"""ClassificationTitanic.ipynb

Automatically generated by Colaboratory.

## **Tabla de contenido:**
* Introducción
* El RMS Titanic
* Importar bibliotecas
* Obtener los datos
* Exploración/Análisis de datos
* Preprocesamiento de datos
    - Datos perdidos
    - Funciones de conversión
    - Creación de categorías
    - Creación de nuevas características
* Construcción de modelos de aprendizaje automático
    - Entrenamiento 8 modelos diferentes
    - ¿Cuál es el mejor modelo?

# **Introducción**

Construtir un modelo de aprendizaje automático con el  conjunto de datos del Titanic, este data set proporciona
información sobre el destino de los pasajeros del Titanic, un resumen según estatus económico (clase), sexo,
edad y supervivencia. En este notebook, debemos pronosticar si un pasajero del Titanic habría sobrevivido o no.

# **El RMS Titanic**

RMS Titanic fue un transatlántico de pasajeros británico que se hundió en el Océano Atlántico Norte en las primeras
horas de la mañana del 15 de abril de 1912, después de chocar con un iceberg durante su viaje inaugural desde
Southampton a la ciudad de Nueva York. Se estima que había 2.224 pasajeros y tripulantes a bordo del barco,
y más de 1.500 murieron, lo que lo convierte en uno de los desastres marítimos comerciales en tiempos de paz más
mortíferos de la historia moderna. El RMS Titanic era el barco más grande a flote en el momento en que entró en
servicio y fue el segundo de los tres transatlánticos de clase olímpica operados por White Star Line. El Titanic fue
construido por el astillero Harland and Wolff en Belfast. Thomas Andrews, su arquitecto, murió en el desastre.

# **Cargando las Librarias**
"""

# Commented out IPython magic to ensure Python compatibility.
# linear algebra
import numpy as np

# data processing
import pandas as pd

# data visualization
import seaborn as sns
# %matplotlib inline
from matplotlib import pyplot as plt

# Algorithms
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Perceptron
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import GaussianNB
import re

"""# **Obteniendo los datos**"""

test_df = pd.read_csv("../../data/test.csv")
train_df = pd.read_csv("../../data/train.csv")

"""# **Exploración/análisis de datos**"""

print(train_df.info())

"""**El conjunto de entrenamiento tiene 891 ejemplos y 11 características + la variable objetivo (survived)**. 2 de 
las características son flotantes, 5 son enteros y 5 son objetos. A continuación las características con una breve 
descripción:

    survival:	Survival
    PassengerId: Unique Id of a passenger.
    pclass:	Ticket class
    sex:	Sex
    Age:	Age in years
    sibsp:	# of siblings / spouses aboard the Titanic
    parch:	# of parents / children aboard the Titanic
    ticket:	Ticket number
    fare:	Passenger fare
    cabin:	Cabin number
    embarked:	Port of Embarkation
"""

print(train_df.describe())

"""Arriba podemos ver que **38% del conjunto de entrenamiento sobrevivió al Titanic**. También podemos ver que las 
edades de los pasajeros oscilan entre 0,4 y 80 años. Además de eso, ya podemos detectar algunas funciones que 
contienen valores faltantes, como la función 'Edad'."""

print(train_df.head(15))

"""De la tabla anterior, podemos notar algunas cosas. En primer lugar, **necesitamos convertir muchas funciones en 
funciones numéricas** más adelante, para que los algoritmos de aprendizaje automático puedan procesarlas. Además, 
podemos ver que las **características tienen rangos muy diferentes**, que necesitaremos convertir aproximadamente a 
la misma escala. También podemos detectar algunas características más, que contienen valores faltantes (NaN).

**Detallando los datos que realmente faltan:**
"""

total = train_df.isnull().sum().sort_values(ascending=False)
percent_1 = train_df.isnull().sum() / train_df.isnull().count() * 100
percent_2 = (round(percent_1, 1)).sort_values(ascending=False)
missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])
print(missing_data.head(5))

"""La función Embarked tiene solo 2 valores faltantes, que se pueden completar fácilmente. Será mucho más complicado 
trabajar con la función 'age', que tiene 177 valores faltantes. La función "Cabin" necesita más investigación, 
pero podríamos eliminarla del conjunto de datos, ya que falta el 77 %."""

print(train_df.columns.values)

"""Arriba puede ver las 11 características + la variable de destino (survived). **¿Qué características podrían 
contribuir a una alta tasa de supervivencia?**

Para mí, tendría sentido si todo excepto 'PassengerId', 'Ticket' y 'Name' se correlacionaran con una alta tasa de 
supervivencia.

**1. Age and Sex:**
"""

survived = 'survived'
not_survived = 'not survived'
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))
women = train_df[train_df['Sex'] == 'female']
men = train_df[train_df['Sex'] == 'male']
ax = sns.histplot(women[women['Survived'] == 1].Age.dropna(), bins=18, label=survived, ax=axes[0], kde=False)
ax = sns.histplot(women[women['Survived'] == 0].Age.dropna(), bins=40, label=not_survived, ax=axes[0], kde=False)
ax.legend()
ax.set_title('Female')
ax = sns.histplot(men[men['Survived'] == 1].Age.dropna(), bins=18, label=survived, ax=axes[1], kde=False)
ax = sns.histplot(men[men['Survived'] == 0].Age.dropna(), bins=40, label=not_survived, ax=axes[1], kde=False)
ax.legend()
_ = ax.set_title('Male')

"""Puede ver que los hombres tienen una alta probabilidad de supervivencia cuando tienen entre 18 y 30 años, 
lo que también es un poco cierto para las mujeres, pero no del todo. Para las mujeres, las posibilidades de 
supervivencia son mayores entre los 14 y los 40 años.

Para los hombres, la probabilidad de supervivencia es muy baja entre los 5 y los 18 años, pero eso no es cierto para 
las mujeres. Otra cosa a tener en cuenta es que los bebés también tienen una probabilidad un poco mayor de 
supervivencia.

Dado que para **ciertas edades, tienen mayores probabilidades de supervivencia** y como queremos que todas las 
características estén aproximadamente en la misma escala, se crean grupos de edad más adelante.

**2. Embarked, Pclass  and Sex:**
"""

FacetGrid = sns.FacetGrid(train_df, row='Embarked', height=4.5, aspect=1.6)
FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='dark:#1f77b4', order=None, hue_order=None)
FacetGrid.add_legend()

"""Embarked parece estar correlacionado con la supervivencia, dependiendo del género.

Las mujeres en el puerto Q y en el puerto S tienen una mayor probabilidad de supervivencia. Lo contrario es cierto, 
si están en el puerto C. Los hombres tienen una alta probabilidad de supervivencia si están en el puerto C, 
pero una probabilidad baja si están en el puerto Q o S.

Pclass también parece estar correlacionado con la supervivencia.

**3. Pclass:**
"""

sns.barplot(x='Pclass', y='Survived', data=train_df)

"""Aquí vemos claramente que Pclass está contribuyendo a la posibilidad de supervivencia de una persona, 
especialmente si esta persona está en la clase 1."""

grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', height=2.2, aspect=1.6)
grid.map(plt.hist, 'Age', alpha=.5, bins=20)
grid.add_legend()

"""El gráfico anterior confirma la suposición sobre la pclass 1, pero también podemos detectar una alta probabilidad 
de que una persona en la pclass 3 no sobreviva.

**4.  SibSp and Parch:**

SibSp y Parch tendrían más sentido como una característica combinada, que muestra el número total de familiares que 
una persona tiene en el Titanic. A continuación se crea y también se agrega una característica que muestre si alguien 
no está solo."""

data = [train_df, test_df]
for dataset in data:
    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']
    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0
    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1
    dataset['not_alone'] = dataset['not_alone'].astype(int)

print(train_df['not_alone'].value_counts())

axes = sns.catplot(x='relatives', y='Survived',
                   data=train_df, aspect=2.5, )

"""Aquí podemos ver que tenían una alta probabilidad de supervivencia con 1 a 3 familiares, pero menor si tenías 
menos de 1 o más de 3 (salvo algunos casos con 6 familiares).

# **Data Preprocessing**

Primero, eliminar 'PassengerId' del conjunto de train, porque no contribuye a la probabilidad de supervivencia de una 
persona. No se eliminara del conjunto de prueba, ya que se requiere allí para el envío."""

train_df = train_df.drop(['PassengerId'], axis=1)

"""## Datos Faltantes:
### Cabin:
Como recordatorio, tenemos que lidiar con Cabin (687), Embarked (2) y Age (177).

Tendriamos que eliminar la variable 'Cabin' pero un número de cabina parece "C123" y la **letra se refiere a la 
cubierta**.

Por lo tanto, vamos a extraerlos y crear una nueva característica que contenga algunas personas. Posteriormente, 
convertiremos la función en una variable numérica. Los valores faltantes se convertirán a cero.

En la imagen a continuación, puede ver las cubiertas reales del Titanic, que van desde la A hasta la G.

![titanic decks](http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Titanic_cutaway_diagram.png/687px
-Titanic_cutaway_diagram.png)"""

deck = {"A": 1, "B": 2, "C": 3, "D": 4, "E": 5, "F": 6, "G": 7, "U": 8}
data = [train_df, test_df]

for dataset in data:
    dataset['Cabin'] = dataset['Cabin'].fillna("U0")
    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile("([a-zA-Z]+)").search(x).group())
    dataset['Deck'] = dataset['Deck'].map(deck)
    dataset['Deck'] = dataset['Deck'].fillna(0)
    dataset['Deck'] = dataset['Deck'].astype(int)

# we can now drop the cabin feature
train_df = train_df.drop(['Cabin'], axis=1)
test_df = test_df.drop(['Cabin'], axis=1)

"""### Age:

Ahora podemos abordar el problema con los valores faltantes de las características de edad. Se crea una matriz que 
contenga números aleatorios, que se calculan en función del valor de la edad media con respecto a la desviación 
estándar y is_null."""

data = [train_df, test_df]

for dataset in data:
    mean = train_df["Age"].mean()
    std = test_df["Age"].std()
    is_null = dataset["Age"].isnull().sum()
    # compute random numbers between the mean, std and is_null
    rand_age = np.random.randint(mean - std, mean + std, size=is_null)
    # fill NaN values in Age column with random values generated
    age_slice = dataset["Age"].copy()
    age_slice[np.isnan(age_slice)] = rand_age
    dataset["Age"] = age_slice
    dataset["Age"] = train_df["Age"].astype(int)

print(train_df["Age"].isnull().sum())

"""### Embarked:

Dado que la función Embarked solo tiene 2 valores faltantes, solo los completaremos con el más común.
"""

print(train_df['Embarked'].describe())

common_value = 'S'
data = [train_df, test_df]

for dataset in data:
    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)

"""## Funciones de conversión:"""

print(train_df.info())

"""Arriba puede ver que 'fare' es un flotante y tenemos que trabajar con 4 características categóricas: Name, Sex, 
Ticket and Embarked Investiguemos y transformemos uno tras otro.

### Fare:

Convirtiendo Fare" de float a int64, utilizando la función "astype()" que proporciona pandas:
"""

data = [train_df, test_df]

for dataset in data:
    dataset['Fare'] = dataset['Fare'].fillna(0)
    dataset['Fare'] = dataset['Fare'].astype(int)

"""### Name:

Usaremos la función Nombre para extraer los Títulos del Nombre, de modo que podamos crear una nueva función a partir 
de eso."""

data = [train_df, test_df]
titles = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5}

for dataset in data:
    # extract titles
    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)
    # replace titles with a more common title or as Rare
    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr',
                                                 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')
    # convert titles into numbers
    dataset['Title'] = dataset['Title'].map(titles)
    # filling NaN with 0, to get safe
    dataset['Title'] = dataset['Title'].fillna(0)

train_df = train_df.drop(['Name'], axis=1)
test_df = test_df.drop(['Name'], axis=1)

"""### Sex:

Convierta la función 'Sex' en numérica.
"""

genders = {"male": 0, "female": 1}
data = [train_df, test_df]

for dataset in data:
    dataset['Sex'] = dataset['Sex'].map(genders)

"""### Ticket:"""

print(train_df['Ticket'].describe())

"""Dado que el atributo Ticket tiene 681 tickets únicos, será un poco complicado convertirlos en categorías útiles. 
Así que lo eliminaremos del conjunto de datos."""

train_df = train_df.drop(['Ticket'], axis=1)
test_df = test_df.drop(['Ticket'], axis=1)

"""### Embarked:

Convierte la característica 'Embarked' en numérica.

"""

ports = {"S": 0, "C": 1, "Q": 2}
data = [train_df, test_df]

for dataset in data:
    dataset['Embarked'] = dataset['Embarked'].map(ports)

"""## Creación de categorías:

Ahora crearemos categorías dentro de las siguientes características:

### Age: Ahora necesitamos convertir la función 'edad'. Primero lo convertiremos de flotante a entero. Luego, 
crearemos la nueva variable 'AgeGroup", categorizando cada edad en un grupo. Tener en cuenta que es importante 
prestar atención a cómo forman estos grupos, ya que no se desea, por ejemplo, que el 80% de sus datos caigan en grupo 
1."""

data = [train_df, test_df]
for dataset in data:
    dataset['Age'] = dataset['Age'].astype(int)
    dataset.loc[dataset['Age'] <= 11, 'Age'] = 0
    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1
    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2
    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3
    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4
    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5
    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6
    dataset.loc[dataset['Age'] > 66, 'Age'] = 6

# let's see how it's distributed
print(train_df['Age'].value_counts())

"""### Fare: Para la función 'Fare', debemos hacer lo mismo que con la función 'Age'. Pero no es tan fácil, 
porque si dividimos el rango de los valores de las tarifas en unas pocas categorías igualmente grandes, el 80% de los 
valores caerían en la primera categoría. Afortunadamente, podemos usar la función sklearn "qcut()", que podemos usar 
para ver cómo podemos formar las categorías."""

print(train_df.head(10))

data = [train_df, test_df]

for dataset in data:
    dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0
    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1
    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2
    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare'] = 3
    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare'] = 4
    dataset.loc[dataset['Fare'] > 250, 'Fare'] = 5
    dataset['Fare'] = dataset['Fare'].astype(int)

"""# Creando nuevas funciones

Agregar dos características nuevas al conjunto de datos, que se calcula a partir de otras características.

### 1. Age times Class
"""

data = [train_df, test_df]
for dataset in data:
    dataset['Age_Class'] = dataset['Age'] * dataset['Pclass']

"""### 2.  Fare per Person"""

for dataset in data:
    dataset['Fare_Per_Person'] = dataset['Fare'] / (dataset['relatives'] + 1)
    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)

# Let's take a last look at the training set, before we start training the models.
print(train_df.head(20))

"""
# **Construcción de modelos de aprendizaje automático**"""

X_train = train_df.drop("Survived", axis=1)
Y_train = train_df["Survived"]
X_test = test_df.drop("PassengerId", axis=1).copy()

# stochastic gradient descent (SGD) learning
sgd = linear_model.SGDClassifier(max_iter=5, tol=None)
sgd.fit(X_train, Y_train)
Y_pred = sgd.predict(X_test)

sgd.score(X_train, Y_train)

acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)

print('SGD: ', round(acc_sgd, 2, ), "%")

# Random Forest
random_forest = RandomForestClassifier(n_estimators=100)
random_forest.fit(X_train, Y_train)

Y_prediction = random_forest.predict(X_test)

random_forest.score(X_train, Y_train)
acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)
print('Random Forest: ', round(acc_random_forest, 2, ), "%")

# Logistic Regression
logreg = LogisticRegression()
logreg.fit(X_train, Y_train)

Y_pred = logreg.predict(X_test)

acc_log = round(logreg.score(X_train, Y_train) * 100, 2)
print('Regression: ', round(acc_log, 2, ), "%")

# KNN
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, Y_train)

Y_pred = knn.predict(X_test)

acc_knn = round(knn.score(X_train, Y_train) * 100, 2)
print('KNN: ', round(acc_knn, 2, ), "%")

# Gaussian Naive Bayes
gaussian = GaussianNB()
gaussian.fit(X_train, Y_train)

Y_pred = gaussian.predict(X_test)

acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)
print('Gaussian Naive Bayes: ', round(acc_gaussian, 2, ), "%")

# Perceptron
perceptron = Perceptron(max_iter=13)
perceptron.fit(X_train, Y_train)

Y_pred = perceptron.predict(X_test)

acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)
print('Perceptron: ', round(acc_perceptron, 2, ), "%")
# Linear SVC
linear_svc = LinearSVC()
linear_svc.fit(X_train, Y_train)

Y_pred = linear_svc.predict(X_test)

acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)
print('Linear SVC: ', round(acc_linear_svc, 2, ), "%")
# Decision Tree
decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, Y_train)

Y_pred = decision_tree.predict(X_test)

acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)
print('Decision Tree: ', round(acc_decision_tree, 2, ), "%")

"""
##  Cual es el mejor modelo ?"""

results = pd.DataFrame({
    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression',
              'Random Forest', 'Naive Bayes', 'Perceptron',
              'Stochastic Gradient Decent',
              'Decision Tree'],
    'Score': [acc_linear_svc, acc_knn, acc_log,
              acc_random_forest, acc_gaussian, acc_perceptron,
              acc_sgd, acc_decision_tree]})
result_df = results.sort_values(by='Score', ascending=False)
result_df = result_df.set_index('Score')
print(result_df.head(9))

"""
Como podemos ver, el clasificador Random Forest es el mejor.


"""
